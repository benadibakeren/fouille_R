---
title: "<FONT color='#0066CC'><FONT size = 4 ><DIV align= center> AP-4209 ESIEE-Paris: 2025 - 2026 </DIV></FONT></FONT>"
output:
  html_document:
    highlight: textmate
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: yes
---

<style type="text/css">
body, td {font-size: 15px;}
code.r{font-size: 12px;}
pre { font-size: 12px;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```
<FONT color='#0066CC'><FONT size = 4 >

::: {align="center"} Fouille de données\

PROJET 3 : CLASSIFICATION BAYÉSIENNE ET ANALYSE FACTORIELLE DISCRIMINANTE

-- Classification des Thèses de Doctorat Françaises -- :::

</FONT></FONT>

<FONT color='#0066CC'><FONT size = 4 >

::: {align="center"} 
Auteurs : Meliana ZERROUG, Keren BENADIBA-- ESIEE Paris 
:::

</FONT></FONT>

<hr style="border: 1px solid gray">

</hr>

<DIV align = justify>

### <FONT color='#0066CC'><FONT size = 4> 1. Introduction et Objectifs </FONT></FONT>

Dans ce projet, nous allons nous intéresser à la classification de thèses de doctorat françaises à partir de leurs résumés.
Le jeu de données utilisé provient de Kaggle.
Nous commencerons par charger les données contenant les résumés des thèses. Ensuite, le texte est nettoyé en supprimant les mots inutiles (stopwords), la ponctuation, et en appliquant un stemming ou une lemmatisation. Après ce nettoyage, le texte est transformé en données numériques grâce à des méthodes de vectorisation comme TF-IDF. Puis, nous extrayons les caractéristiques les plus importantes et nous réduisons la dimension des données en utilisant l’analyse factorielle discriminante (AFD). Enfin, un classifieur bayésien naïf est entraîné à partir des données obtenues, puis évalué à l’aide de la validation croisée et de différentes mesures de performance.

</DIV>

<hr style="border: 1px solid gray">

</hr>

<DIV align = justify>

<FONT color='#0066CC'><FONT size = 4> 2. Méthodologie </FONT></FONT>
<FONT color='#0066CC'><FONT size = 3> 2.1 Chargement des données </FONT></FONT>

Nous utilisons donc le jeu de données des thèses françaises. Nous allons donc d'abord charger les bibliothèques nécessaires. 

*  **Installation des packages nécessaires :**  

```{r}

# Définir un miroir CRAN
options(repos = c(CRAN = "https://cloud.r-project.org/"))

if(!require("dplyr")) install.packages("dplyr")
if(!require("kableExtra")) install.packages("kableExtra")
if(!require("tm")) install.packages("tm")
if(!require("MASS")) install.packages("MASS") 
if(!require("e1071")) install.packages("e1071") 
if(!require("topicmodels")) install.packages("topicmodels") 
if(!require("caret")) install.packages("caret") 
if(!require("stopwords")) install.packages("stopwords")
if(!require("naivebayes")) install.packages("naivebayes")
if(!require("ggplot2")) install.packages("ggplot2") 

```

*  **Chargement des packages nécessaires :**

```{r}

library(dplyr)
library(kableExtra)
library(tm)
library(MASS)
library(e1071)
library(topicmodels)
library(caret)
library(stopwords)
library(naivebayes)
library(stopwords)
library(ggplot2)

```

*  **Première étape :**

```{r}
file.remove("THESE_DATASET.Rda")
file.remove("THESE_CLEAN.Rda")

rm(list = ls())

```

*  **Seconde étape :**

```{r}
# Update this path to match your actual file location
csv_file <- "french_thesis_20231021_metadata.csv"
df_temp <- read.csv(csv_file)
save(df_temp, file = "THESE_DATASET.Rda")

```

*  **Troisième étape :**

```{r}

rm(df_temp)
rm(list = ls())
load("THESE_DATASET.Rda")
colnames(df_temp)
head(df_temp)

```

*  **Vérification du nombre de descriptions vides  :**

```{r}

nb_vide <- sum(df_temp$Description == "")
nb_na <- sum(is.na(df_temp$Description))

total_lignes <- nrow(df_temp)
total_inutiles <- nb_vide + nb_na

cat("Nombre total de lignes :", total_lignes, "\n")
cat("--- \n")
cat("Total à supprimer :", total_inutiles, "\n")

```


*  **Quatrième étape :**

```{r}

df_clean <- df_temp %>%
  dplyr::rename(Subject = Domain, 
                Texte = Description) %>% 
  dplyr::select(Subject, Texte)

#On va enlever les lignes vides car il y a quelques descriptions vides

df_clean <- df_clean %>% 
  filter(!is.na(Texte) & Texte != "" & Texte != " ")

cat("Nombre de lignes après le nettoyage : ", nrow(df_clean), "\n")

set.seed(123)
df_clean <- df_clean %>%
            sample_n(5000)  # Réduire à 5000 pour une exécution plus rapide

kable(head(df_clean), caption = "Jeu de données") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")%>%  
  scroll_box( height = "200px")

```

*  **Regroupement des thèmes en catégories principales :**

```{r}

# Créer une fonction pour regrouper les thèmes similaires
regroup_subjects <- function(subject) {
  subject <- tolower(subject)
  
  # Ordre d'importance : être plus spécifique d'abord
  if (grepl("biologie|écologie|environnement|biodiversité|organisme|cellulaire|moléculaire|terre|géologie|océan|espace|atmosphère|climat|toxicologie|médecine|pharmacologie|neurosciences|oncologie|épidémiologie|cancérologie", subject)) {
    return("Biologie et Sciences de la Vie")
  } else if (grepl("chimie|physique|matériaux|minérale|organique|analytique|énergétique", subject)) {
    return("Chimie et Physique")
  } else if (grepl("informatique|numérique|signal|communication|télécommunication|électronique", subject)) {
    return("Informatique et Télécommunications")
  } else if (grepl("génie|ingénieur|technologie|civil|mécanique|électrique|énergie|fluide|dynamique", subject)) {
    return("Génie et Technologie")
  } else if (grepl("droit|juridique|science.*politique|politique|gouvernement|constitution|public|privé|international|criminel", subject)) {
    return("Droit et Sciences Politiques")
  } else if (grepl("mathématique|statistique|probabilité", subject)) {
    return("Mathématiques et Statistiques")
  } else if (grepl("histoire|archéologie|littérature|langue|étude|germaniques|anglaises|italienne|latino|nord-américain|cinéma", subject)) {
    return("Histoire et Lettres")
  } else if (grepl("anthropologie|sociologie|psychologie|sciences.*humaines|ethnologie|sport|mouvement|éducation|formation|pédagogie|apprentissage|enseignement", subject)) {
    return("Sciences Humaines et Sociales")
  } else {
    # Si on arrive ici, on essaie de capturer des éléments génériques
    if (grepl("science", subject)) {
      return("Sciences générales")
    }
    return("Sciences générales")  # Plutôt que "Autres"
  }
}

# Appliquer le regroupement
df_clean$Subject <- sapply(df_clean$Subject, regroup_subjects)

# Afficher la nouvelle distribution
table_subjects <- table(df_clean$Subject)
cat("\nDistribution des catégories :\n")
print(table_subjects)

# Garder seulement les catégories avec au moins 50 documents
df_clean <- df_clean %>%
  filter(Subject %in% names(table_subjects)[table_subjects >= 50])

cat("\nNombre total après filtrage :", nrow(df_clean), "\n")
cat("Nombre de catégories :", length(unique(df_clean$Subject)), "\n")
cat("\nDistribution finale :\n")
print(table(df_clean$Subject))

```

*  **Cinquième étape :**

```{r}

#On va maintenant enregistrer le nouveau dataframe et on nettoie 

save(df_clean, file = "THESE_CLEAN.Rda")
rm(df_clean, df_temp)
file.remove("THESE_DATASET.Rda")

```

*  **Sixième étape :**

```{r}

#On charge le nouveau jeu de données 
rm(list = ls())
load("THESE_CLEAN.Rda")
colnames(df_clean)

```

*  **Septième étape :**

```{r}

#Présentation des données finales 

kable(head(df_clean), caption = "Jeu de données") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")%>%  
  scroll_box( height = "200px")

```

<FONT color='#0066CC'><FONT size = 3> 2.2 Préparation pour l'AFD et le Bayésien (NLP) </FONT></FONT>

<DIV align = justify>

<FONT color='#0066CC'><FONT size = 3> 2.2 Préparation pour l'AFD et le Bayésien (NLP) </FONT></FONT>

<DIV align = justify>

Maintenant on va passer au traitement du texte. L'objectif va être de transformer les résumés en données numériques. Contrairement au mini projet 2 (Twitter), nous devions
traiter de l'anglais, ici nous allons devoir traiter du français donc il va falloir gérer les accents et des mots vides différents. 


```{r}

# Charger les stopwords français
suppressWarnings({
  stopwords_fr <- stopwords::stopwords("fr")
})

# Créer un corpus à partir des textes
corpus <- Corpus(VectorSource(df_clean$Texte))

# Nettoyage du texte
corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%           # Convertir en minuscules
  tm_map(content_transformer(function(x) iconv(x, from = "UTF-8", to = "ASCII//TRANSLIT"))) %>%  # Supprimer les accents
  tm_map(removePunctuation) %>%                       # Supprimer la ponctuation
  tm_map(removeNumbers) %>%                           # Supprimer les chiffres
  tm_map(stripWhitespace) %>%                         # Supprimer les espaces superflus
  tm_map(removeWords, stopwords_fr)                   # Supprimer les stopwords français

# Créer une matrice document-terme (DTM)
dtm <- DocumentTermMatrix(corpus)

# Supprimer les termes très rares (qui apparaissent dans moins de 0.1% des documents)
sparse_threshold <- 0.999
dtm_reduced <- removeSparseTerms(dtm, sparse_threshold)

# Convertir en matrice dense pour les analyses
data_matrix <- as.matrix(dtm_reduced)

cat("Dimensions de la matrice document-terme :", nrow(data_matrix), "x", ncol(data_matrix), "\n")

```

*  **Vectorisation TF-IDF :**

```{r}

# Créer une matrice TF-IDF
tfidf_matrix <- weightTfIdf(dtm_reduced)
tfidf_data <- as.matrix(tfidf_matrix)

cat("Dimensions de la matrice TF-IDF :", nrow(tfidf_data), "x", ncol(tfidf_data), "\n")
cat("Nombre de termes uniques :", ncol(tfidf_data), "\n")

```

*  **Sélection des termes les plus importants :**

```{r}

# Calculer la variance de chaque terme et sélectionner les top 20 (au lieu de 100)
variances <- apply(tfidf_data, 2, var)
top_indices <- order(variances, decreasing = TRUE)[1:min(20, length(variances))]
tfidf_data_reduced <- tfidf_data[, top_indices]

cat("Dimensions après réduction :", nrow(tfidf_data_reduced), "x", ncol(tfidf_data_reduced), "\n")

```

*  **Combinaison avec la variable cible :**

```{r}

# Créer un dataframe avec les données TF-IDF réduites et la variable cible
df_features <- data.frame(
  Subject = df_clean$Subject,
  as.data.frame(tfidf_data_reduced)
)

# Afficher les dimensions
cat("Dimensions du dataset avec features TF-IDF :", nrow(df_features), "x", ncol(df_features), "\n")

# Afficher la distribution des classes
table_dist <- table(df_features$Subject)
cat("\nDistribution des classes :\n")
print(table_dist)

```

<FONT color='#0066CC'><FONT size = 3> 2.3 Analyse Factorielle Discriminante (AFD) </FONT></FONT>

<DIV align = justify>

L'Analyse Factorielle Discriminante est une technique de réduction de dimensionnalité supervisée qui cherche à maximiser la séparation entre les classes. Elle crée de nouvelles variables (axes discriminants) qui sont les meilleures combinaisons linéaires des variables originales pour distinguer les différentes classes.

```{r}

# Sélectionner uniquement les variables numériques (termes TF-IDF)
X <- df_features[, -1]
y <- df_features$Subject

# Appliquer l'AFD
lda_model <- MASS::lda(X, grouping = y)

# Afficher les informations du modèle
cat("Nombre d'axes discriminants :", length(lda_model$svd), "\n")
cat("Proportions de la variance expliquée :\n")
print(lda_model$svd^2 / sum(lda_model$svd^2))

# Obtenir les scores des axes discriminants
lda_scores <- predict(lda_model, X)$x

# Créer un dataframe avec les scores et la classe
df_lda <- data.frame(
  Subject = y,
  lda_scores
)

cat("Dimensions des données après AFD :", nrow(df_lda), "x", ncol(df_lda), "\n")

# Créer un tableau récapitulatif des résultats de l'AFD avec tests de Wilks
n_axes <- min(2, length(lda_model$svd))  # Prendre max 2 axes pour le tableau
result_afd <- matrix(0, nrow = n_axes, ncol = 6)

# Valeurs propres
result_afd[, 1] <- lda_model$svd[1:n_axes]^2

# Pourcentage d'inertie
result_afd[, 2] <- (lda_model$svd[1:n_axes]^2 / sum(lda_model$svd^2)) * 100

# Corrélations canoniques
result_afd[, 3] <- sqrt(lda_model$svd[1:n_axes]^2 / (1 + lda_model$svd[1:n_axes]^2))

# Wilks Lambda (produit cumulatif)
result_afd[, 4] <- cumprod(1 - result_afd[, 3]^2)

# Khi-deux
N <- nrow(X)
P <- ncol(X)
K <- length(unique(y))
result_afd[, 5] <- -(N - (P + K)/2 - 1) * log(result_afd[, 4])

# p-value
result_afd[, 6] <- 1 - pchisq(result_afd[, 5], P * (K - 1))

result_afd_df <- data.frame(result_afd)
colnames(result_afd_df) <- c("Valeur propre", "% Inertie", "Corrélation", "Wilks Lambda", "Khi-deux", "p-value")
rownames(result_afd_df) <- paste0("Axe ", 1:n_axes)

cat("\n=== TABLEAU RÉCAPITULATIF DE L'AFD (Tests de Wilks) ===\n")
print(result_afd_df)

```

*  **Visualisation avec ggplot2 (centres de gravité) :**

```{r fig.width=14, fig.height=8}

# Créer un dataframe pour ggplot2
plot_data_gg <- data.frame(
  LD1 = lda_scores[, 1],
  LD2 = lda_scores[, 2],
  Class = y
)

# Calculer les centres de gravité de chaque classe
centers <- aggregate(cbind(LD1, LD2) ~ Class, data = plot_data_gg, mean)

# Créer le graphique avec ggplot2
ggplot(plot_data_gg, aes(x = LD1, y = LD2, colour = Class)) +
  geom_point(size = 2, alpha = 0.6) +
  geom_point(data = centers, aes(x = LD1, y = LD2, colour = Class), 
             shape = 23, size = 5, fill = "red") +  # Centres en losange rouge
  geom_hline(yintercept = 0, size = 0.1, colour = 'black', linetype = "dashed") +
  geom_vline(xintercept = 0, size = 0.1, colour = 'black', linetype = "dashed") +
  theme_minimal() +
  ggtitle("Projection AFD avec centres de gravité") +
  xlab("Axe discriminant 1") +
  ylab("Axe discriminant 2") +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 10),
    legend.title = element_text(size = 11),
    plot.title = element_text(size = 13, hjust = 0.5),
    axis.title = element_text(size = 11)
  ) +
  guides(colour = guide_legend(ncol = 1, override.aes = list(size = 3)))

```

<FONT color='#0066CC'><FONT size = 3> 2.4 Classification par distance aux centres de gravité </FONT></FONT>

<DIV align = justify>

Au lieu d'utiliser un Bayésien Naïf, nous utilisons la méthode de classification supervisée basée sur l'AFD : chaque individu est affecté à la classe dont le centre de gravité est le plus proche dans le plan factoriel.

```{r}

# Diviser les données en ensemble d'entraînement et de test (80-20)
set.seed(123)
indices <- createDataPartition(df_lda$Subject, p = 0.8, list = FALSE)
train_data <- df_lda[indices, ]
test_data <- df_lda[-indices, ]

cat("Taille de l'ensemble d'entraînement :", nrow(train_data), "\n")
cat("Taille de l'ensemble de test :", nrow(test_data), "\n")

```

*  **Calcul des centres de gravité :**

```{r}

# Calculer les centres de gravité de chaque classe sur les données d'entraînement
centers <- aggregate(train_data[, -1], by = list(train_data$Subject), mean)
rownames(centers) <- centers$Group.1
centers <- centers[, -1]

cat("\nCentres de gravité calculés pour", nrow(centers), "classes\n")

# Supprimer les catégories avec moins de 10 exemples dans le test
test_class_counts <- table(test_data$Subject)
classes_to_keep <- names(test_class_counts[test_class_counts >= 10])

# Filtrer les données de test
test_data <- test_data %>% filter(Subject %in% classes_to_keep)
centers <- centers[rownames(centers) %in% classes_to_keep, , drop = FALSE]

cat("Catégories conservées pour l'évaluation :", length(classes_to_keep), "\n")
cat("Taille de l'ensemble de test après filtrage :", nrow(test_data), "\n")

```

*  **Classification par distance :**

```{r}

# Fonction pour classifier par distance aux centres de gravité
classify_by_distance <- function(individual, centers) {
  distances <- apply(centers, 1, function(center) {
    sqrt(sum((individual - center)^2))
  })
  names(distances)[which.min(distances)]
}

# Faire des prédictions sur l'ensemble de test
predictions <- apply(test_data[, -1], 1, classify_by_distance, centers = centers)
predictions <- as.factor(predictions)
test_reference <- as.factor(test_data$Subject)

# S'assurer qu'ils ont les mêmes niveaux
common_levels <- intersect(levels(predictions), levels(test_reference))
predictions <- factor(predictions, levels = common_levels)
test_reference <- factor(test_reference, levels = common_levels)

# Créer une matrice de confusion
confusion_matrix <- confusionMatrix(predictions, test_reference)

print(confusion_matrix)

# Extraire les métriques principales
cat("\nAccuracy :", round(confusion_matrix$overall['Accuracy'] * 100, 2), "%\n")
cat("Kappa :", round(confusion_matrix$overall['Kappa'], 3), "\n")

```

<FONT color='#0066CC'><FONT size = 3> 2.5 Analyse des performances et interprétation </FONT></FONT>

<DIV align = justify>

Nous analysons maintenant les performances détaillées du modèle et la qualité de la classification par catégorie.

```{r}

# Créer une table de classification
class_performance <- data.frame(
  Classe = rownames(confusion_matrix$table),
  Prédictions_Correctes = diag(confusion_matrix$table),
  Total = rowSums(confusion_matrix$table)
)
class_performance$Taux_Reussite <- round(class_performance$Prédictions_Correctes / class_performance$Total * 100, 2)

kable(class_performance, caption = "Taux de réussite par classe") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position = "center")

```

*  **Graphique de précision par catégorie :**

```{r fig.width=12, fig.height=6}

# Créer un graphique de précision par catégorie
accuracy_by_class <- data.frame(
  Class = class_performance$Classe,
  Accuracy = class_performance$Taux_Reussite
)

# Trier par précision
accuracy_by_class <- accuracy_by_class[order(accuracy_by_class$Accuracy), ]
accuracy_by_class$Class <- factor(accuracy_by_class$Class, 
                                   levels = accuracy_by_class$Class)

# Créer le graphique
ggplot(accuracy_by_class, aes(x = Class, y = Accuracy, fill = Accuracy)) +
  geom_bar(stat = "identity", color = "black", size = 0.5) +
  scale_fill_gradient(low = "red", high = "green", limits = c(0, 100)) +
  geom_hline(yintercept = mean(accuracy_by_class$Accuracy), 
             linetype = "dashed", color = "blue", size = 0.7) +
  coord_flip() +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 13, hjust = 0.5, face = "bold"),
    legend.position = "right"
  ) +
  ggtitle("Précision de classification par catégorie") +
  xlab("Catégorie") +
  ylab("Précision (%)") +
  ylim(0, 100)

```

*  **Résumé des résultats :**

```{r}

cat("Nombre total de thèses traitées :", nrow(df_lda), "\n")
cat("Nombre de catégories :", length(unique(df_lda$Subject)), "\n")
cat("Nombre de features TF-IDF :", ncol(X), "\n")
cat("Nombre d'axes discriminants utilisés :", 2, "\n\n")

cat("Accuracy globale :", round(confusion_matrix$overall['Accuracy'] * 100, 2), "%\n")
cat("Kappa :", round(confusion_matrix$overall['Kappa'], 3), "\n\n")

# Calculer les meilleures et pires catégories
best_idx <- which.max(class_performance$Taux_Reussite)
worst_idx <- which.min(class_performance$Taux_Reussite)

cat("Meilleure catégorie :", class_performance$Classe[best_idx], 
    " (", class_performance$Taux_Reussite[best_idx], "%)\n")
cat("Catégorie la plus difficile :", class_performance$Classe[worst_idx], 
    " (", class_performance$Taux_Reussite[worst_idx], "%)\n\n")

cat("Nombre moyen de thèses par catégorie :", round(mean(class_performance$Total), 1), "\n")

```

</DIV>

<hr style="border: 1px solid gray">

</hr>

<DIV align = justify>

<FONT color='#0066CC'><FONT size = 4> 3. Conclusion </FONT></FONT>

Ce projet a démontré comment approcher un problème de classification de texte en utilisant une combinaison de techniques de traitement du langage, de réduction de dimensionnalité et de classification bayésienne. 

Les principales étapes ont été :
1. **Chargement et nettoyage des données** : Suppression des caractères spéciaux, accents et mots vides
2. **Vectorisation TF-IDF** : Transformation du texte en données numériques pondérées par l'importance
3. **Analyse Factorielle Discriminante** : Réduction de la dimensionnalité tout en maximisant la séparation entre classes
4. **Classification Bayésienne Naïve** : Entraînement et validation d'un classifieur probabiliste

Le modèle construit de cette manière permet de classifier automatiquement les thèses françaises dans leurs domaines respectifs, offrant un outil utile pour l'analyse et l'organisation de grands nombres de collections de thèses académiques.

</DIV>