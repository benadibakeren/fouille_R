---
title: "<FONT color='#0066CC'><FONT size = 4 ><DIV align= center> AP-4209 ESIEE-Paris: 2025 - 2026 </DIV></FONT></FONT>"
output:
  html_document:
    highlight: textmate
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: yes
---

<style type="text/css">
body, td {font-size: 15px;}
code.r{font-size: 12px;}
pre { font-size: 12px;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```
<FONT color='#0066CC'><FONT size = 4 >

::: {align="center"} 
Fouille de données\

PROJET FINAL : CLASSIFICATION BAYÉSIENNE ET ANALYSE FACTORIELLE DISCRIMINANTE 
:::

</FONT></FONT>

<FONT color='#0066CC'><FONT size = 4 >

::: {align="center"} 
Auteur : Meliana ZERROUG, Keren BENADIBA-- ESIEE Paris 
:::

</FONT></FONT>

<hr style="border: 1px solid gray">

</hr>

<DIV align = justify>

### <FONT color='#0066CC'><FONT size = 4> 1. Introduction </FONT></FONT>

De nos jours, nous avons de plus en plus d'outils comme ChatGPT ou encore Gemini, et donc c'est de plus en plus compliqué de savoir si un texte a été écrit par un être humain ou bien par une 
machine. C'est dans ce contexte que nous allons réaliser notre projet final qui consiste à essayer de déterminer lorsqu'un texte a été généré par une IA. 

Pour réaliser ce travail nous allons nous baser sur le jeu de données contenu dans le dossier "llm-detect-ai-generated-text", si besoin nous utiliserons des données supplémentaires contenu dans les dossiers "train_drcat".

L'objectif de notre projet est donc de développer un système de classification de texte en combinant deux méthodes que nous avons étudiées ; tout d'abord nous allons utiliser l'analyse factorielle discriminante qui va nous permettre de faire le tri dans les données et les caractéristiques afin de mieux séparer nos deux groupes (humain et IA).
Ensuite, nous utiliserons la classification Bayésienne qui en utilisant les résultats de l'AFD va calculer les probabilités qu'un texte soit généré par un humain ou une IA. 

</DIV>

<hr style="border: 1px  solid gray">

</hr>

<DIV align = justify>

### <FONT color='#0066CC'><FONT size = 4> 2. Données et prétraitement </FONT></FONT>

Nous allons donc utiliser le jeu de données qui provient de Kaggle LLM - Detect AI Generated Text. Nous allons principalement travailler avec le fichier train_essays.csv.
Nous avons également le fichier train_prompts.csv qui va nous fournir le contexte des textes. 

*  **Installation des packages nécessaires :**  

```{r}

if(!require("dplyr")) install.packages("dplyr")
if(!require("kableExtra")) install.packages("kableExtra")
if(!require("tm")) install.packages("tm")
if(!require("MASS")) install.packages("MASS") 
if(!require("e1071")) install.packages("e1071") 
if(!require("topicmodels")) install.packages("topicmodels") 
if(!require("caret")) install.packages("caret") 
if(!require("stopwords")) install.packages("stopwords")
if(!require("naivebayes")) install.packages("naivebayes")
if(!require("ggplot2")) install.packages("ggplot2") 

```

*  **Chargement des packages nécessaires :**

```{r}

library(dplyr)
library(kableExtra)
library(tm)
library(MASS)
library(e1071)
library(topicmodels)
library(caret)
library(stopwords)
library(naivebayes)
library(stopwords)
library(ggplot2)

```

*  **Première étape :**

```{r}
file.remove("TEXT_DATASET.Rda")
file.remove("TEXT_CLEAN.Rda")

rm(list = ls())

```

*  **Seconde étape :**

```{r}

data_dir <- "llm-detect-ai-generated-text"

train_path <- file.path(data_dir, "train_essays.csv")
prompts_path <- file.path(data_dir, "train_prompts.csv")

train_df <- read.csv(train_path)
prompts_df <- read.csv(prompts_path)

save(train_df, prompts_df, file = "TEXT_DATASET.Rda")

```

*  **Troisième étape :**

Nous allons maintenant recharger le fichier créée à l'étape précédente afin de vérifier que les données soient correctement sauvegardées ensuite nous afficherons les dimensions de notre jeu de données. 

```{r}

rm(train_df, prompts_df)
rm(list = ls())
load("TEXT_DATASET.Rda")
colnames(train_df)
kable(head(train_df), caption = "Jeu de données") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")%>%  
  scroll_box( height = "200px")

```

*  **Vérification des données manquantes et de l'équilibre entre les classes :**

Avant de nettoyer, nous allons regarder s'il y a beacoup de données manquantes et nous allons également regarder comment sont réparties nos deux classes (humain et IA). 

```{r}

nb_vide <- sum(train_df$text == "")
nb_na <- sum(is.na(train_df$text))

total_lignes <- nrow(train_df)
total_inutiles <- nb_vide + nb_na

cat("Nombre total de lignes :", total_lignes, "\n")
cat("Total à supprimer :", total_inutiles, "\n")

kable(table(train_df$generated), caption = "Distribution des classes (0=Humain, 1=IA)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")%>%  
  scroll_box( height = "200px")
```

On remarque qu'il n'y a pas de lignes vides à supprimer, cependant on remarque que c'est très déséquilibré on possède 1375 textes écrit par des humain et 3 par des IA. 

*  **Chargement et augmentation des données :**

Comme le jeu de données original est très déséquilibré, nous allons ajouter des données externes (drcat), cela va nous permettre d'avoir assez d'exemples de textes générés par une IA afin de pouvoir entraîner nos modèles. 

```{r}

drcat_paths <- c(
  "train_drcat_01.csv/train_drcat_01.csv",
  "train_drcat_02.csv/train_drcat_02.csv",
  "train_drcat_03.csv/train_drcat_03.csv",
  "train_drcat_04.csv/train_drcat_04.csv"
)

drcat_list <- lapply(drcat_paths, read.csv)
drcat_df <- bind_rows(drcat_list) #permet de prendre la liste entière au liue de coller ls tableaux un par un et créer un seul grand dataframe 

#essayer de rendre la fusion plus simple en harmonisant noms des colonnes (car pas les memes noms de colonnes)
kaggle_df <- train_df %>%
  transmute(
    text = text,
    label = generated,
    source = "kaggle",
    prompt = as.character(prompt_id)
  )

drcat_df <- drcat_df %>%
  transmute(
    text = text,
    label = label,
    source = paste0("drcat_", source),
    prompt = as.character(prompt)
  )


full_data <- bind_rows(kaggle_df, drcat_df)
save(full_data, file = "TEXT_DATASET_AUG.Rda")

kable(table(full_data$label), caption = "Distribution des classes (0=Humain, 1=IA)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")%>%  
  scroll_box( height = "200px")

```

*  **Quatrième étape :**

Nous possèdons maintenant une très grande quantité de données, afin d'optimiser notre temps de calcul nous allons utiliser un échantillon de notre jeu de données.

Nous allons prendre 10 000 textes humains et 10 000 textes fait par une IA (50/50).

```{r}

set.seed(123) 

# Humain
df_humain <- full_data %>% 
  filter(label == 0) %>% 
  sample_n(10000)

# IA 
df_ia <- full_data %>% 
  filter(label == 1) %>% 
  sample_n(10000)

# On fusionne les deux échantillons 
df <- bind_rows(df_humain, df_ia)

df <- df %>%
  dplyr::rename(generated = label) %>%  
  dplyr::select(generated, text) %>%
  mutate(generated = factor(generated, levels = c(0, 1), labels = c("Humain", "IA")))

kable(table(df$generated), caption = "Distribution") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")%>%  
  scroll_box( height = "200px")

```

*  **Cinquième étape :**

Nous allons maintenant sauvegarder cet échantillon du jeu de données dans le fichier TEXT_CLEAN.Rda.

```{r,   warning = F}
# On enregistre le nouveau dataframe transformé et on nettoie
save(df, file = "TEXT_CLEAN.Rda")
rm(df, full_data, df_humain, df_ia, drcat_list)
file.remove("TEXT_DATASET.Rda") # on supprime l'ancien fichier
file.remove("TEXT_DATASET_AUG.Rda")
```

*  **Sixième étape :**

On va maintenant recharger le fichier propre pour être sûr que les données sont prêtes pour la partie 3.

```{r,echo = T,  warning = F}
# On charge le nouveau jeu de données 
rm(list = ls()) # supression de tous les objets en mémoire
load("TEXT_CLEAN.Rda")

colnames(df)

```

*  **Septième étape :**

On va maintenant visualiser les données. 

```{r,   warning = F}

kable(head(df,100), caption = "Extrait du jeu de données") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center") %>%
  scroll_box(height = "400px", width = "100%")

```

</DIV>

<hr style="border: 1px  solid gray">
</hr>

<DIV align = justify>

### <FONT color='#0066CC'><FONT size = 4> 3. Extraction de caractéristiques </FONT></FONT>

Nous allons maintenant transformer les textes en données numériques exploitables pour nos modèles. Pour cela, nous utiliserons la vectorisation TF-IDF (Term Frequency-Inverse Document Frequency) après un nettoyage adéquat du texte.

*  **Nettoyage du texte :**

```{r}

# On charge les stopwords en anglais
suppressWarnings({
  stopwords_en <- stopwords::stopwords("en")
})

# On crée un corpus à partir des textes
corpus <- Corpus(VectorSource(df$text))

# On nettoie le texte
corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%           # Convertie en minuscules
  tm_map(removePunctuation) %>%                       # Supprime la ponctuation
  tm_map(removeNumbers) %>%                           # Supprime les chiffres
  tm_map(stripWhitespace) %>%                         # Supprime les espaces superflus
  tm_map(removeWords, stopwords_en)                   # Supprime les stopwords anglais

dtm <- DocumentTermMatrix(corpus)

# On supprime les termes très rares
sparse_threshold <- 0.999
dtm_reduced <- removeSparseTerms(dtm, sparse_threshold)

data_matrix <- as.matrix(dtm_reduced)

cat("Dimensions de la matrice document-terme :", nrow(data_matrix), "x", ncol(data_matrix), "\n")

```

*  **Vectorisation TF-IDF :**

```{r}

# On crée une matrice TF-IDF
tfidf_matrix <- weightTfIdf(dtm_reduced)
tfidf_data <- as.matrix(tfidf_matrix)

cat("Dimensions de la matrice TF-IDF :", nrow(tfidf_data), "x", ncol(tfidf_data), "\n")
cat("Nombre de termes uniques :", ncol(tfidf_data), "\n")

```

*  **Sélection des termes les plus importants :**

```{r}

total_mots_depart <- ncol(tfidf_data)
variances <- apply(tfidf_data, 2, var)
top_indices <- order(variances, decreasing = TRUE)[1:min(500, length(variances))]
tfidf_data_reduced <- tfidf_data[, top_indices]

cat("Dimensions après réduction :", nrow(tfidf_data_reduced), "x", ncol(tfidf_data_reduced), "\n")
cat("Nombre de mots total après nettoyage :", total_mots_depart, "\n")
cat("Nombre de mots gardés pour l'analyse :", ncol(tfidf_data_reduced), "\n")
```

Au départ notre matrice contenait des milliers de termes, pour éviter de saturer le modèle et éviter trop de bruits inutiles, nous avons décidé de ne garder que les 500 mots qui ont la plus forte variance
cela permet de se concentrer sur les mots les plus parlants en quelque sorte afin de différencier l'IA de l'Humain. 

*  **Combinaison avec la variable cible :**

```{r}

df_features <- data.frame(
  generated = df$generated,
  as.data.frame(tfidf_data_reduced)
)

cat("Dimensions du dataset avec features TF-IDF :", nrow(df_features), "x", ncol(df_features), "\n")

table_dist <- table(df_features$generated)
cat("\nDistribution des classes :\n")
print(table_dist)

```

Pour transformer nos textes en chiffres on a utilisé la méthode TF-IDF. Cetet méthode permet de repérer les mots qui sont vraiment importants dans un texte tout en ignorant les mots trop communs.
Dans notre code, on garde que les 500 mots les plus "discriminants" afin d'aider notre modèle à faire la différence. 

</DIV>

<hr style="border: 1px solid gray">

</hr>

### <FONT color='#0066CC'><FONT size = 4> 4. Analyse Factorielle Discriminante (AFD) </FONT></FONT>

<DIV align = justify>

L'Analyse Factorielle Discriminante (AFD) est une méthode qui va nous permettre de réduire le nombre de variables tout en gardant les informations importantes pour distinguer nos deux groupes (IA et Humain). 
Pour faire simple, on va chercher les meilleures combinaisons des variables originales pour pouvoir séparer au mieux les textes humains des textes générés par l'IA.

Comme notre problème est binaire (Humain ou IA), l'AFD est très adaptée. Elle va nous permettre d'identifier les caractéristiques du texte qui différencient vraiment les deux groupes.

```{r}

# On sélectionne les variables numériques 
X <- df_features[, -1]
y <- df_features$generated

# On applique l'AFD
lda_model <- MASS::lda(X, grouping = y)

cat("Nombre d'axes discriminants :", length(lda_model$svd), "\n")
cat("Proportion de la variance expliquée :\n")
print(lda_model$svd^2 / sum(lda_model$svd^2))

lda_scores <- predict(lda_model, X)$x

# On crée un dataframe avec les scores et la classe
df_lda <- data.frame(
  generated = y,
  lda_scores
)

cat("Dimensions des données après AFD :", nrow(df_lda), "x", ncol(df_lda), "\n")

```

*  **Tableau récapitulatif des résultats de l'AFD :**

```{r}

n_axes <- length(lda_model$svd)  
result_afd <- matrix(0, nrow = n_axes, ncol = 6)

# Valeurs propres
result_afd[, 1] <- lda_model$svd^2

# Pourcentage d'inertie
result_afd[, 2] <- (lda_model$svd^2 / sum(lda_model$svd^2)) * 100

# Corrélations canoniques
result_afd[, 3] <- sqrt(lda_model$svd^2 / (1 + lda_model$svd^2))

# Wilks Lambda 
result_afd[, 4] <- cumprod(1 - result_afd[, 3]^2)

# Khi-deux
N <- nrow(X)
P <- ncol(X)
K <- length(unique(y))
result_afd[, 5] <- -(N - (P + K)/2 - 1) * log(result_afd[, 4])

# p-value
result_afd[, 6] <- 1 - pchisq(result_afd[, 5], P * (K - 1))

result_afd_df <- data.frame(result_afd)
colnames(result_afd_df) <- c("Valeur propre", "% Inertie", "Corrélation", "Wilks Lambda", "Khi-deux", "p-value")
rownames(result_afd_df) <- paste0("Axe ", 1:n_axes)

kable(result_afd_df, caption = "Tests de Wilks") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")
```

*  **Visualisation de la projection AFD :**

```{r fig.width=14, fig.height=8}

# On utilise le premier axe car on est dans un problème binaire
plot_data_gg <- data.frame(
  LD1 = lda_scores[, 1],
  Class = y
)

# On calcule les centres de gravité de chaque classe
centers <- aggregate(LD1 ~ Class, data = plot_data_gg, mean)

ggplot(plot_data_gg, aes(x = LD1, fill = Class)) +
  geom_density(alpha = 0.6, color = "black") +
  geom_vline(data = centers, aes(xintercept = LD1, color = Class), 
             size = 1, linetype = "dashed") +
  theme_minimal() +
  ggtitle("Projection AFD : Séparation Humain vs IA") +
  xlab("Axe discriminant 1") +
  ylab("Densité") +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 10),
    legend.title = element_text(size = 11),
    plot.title = element_text(size = 13, hjust = 0.5),
    axis.title = element_text(size = 11)
  )

```

Sur le graphique obtenu on peut observer deux "montagnes", on remarque qu'il y a un  chevauchement c'est l'endroit où notre modèle va faire des erreurs de confusion.

Pour ce qui est du test de Wilks, on remarque qu'on obtient une valeur propre élevée, ce qui montre que l'axe LD1 explique bien la différence entre nos deux groupes. Le Wilks Lambda est très petit cela montre que les variables choisies sont pertinentes pour classer les textes. 
La p-value est très petite aussi cela prouve que nos résultats ne sont pas dus au hasard. 


</DIV>

<hr style="border: 1px solid gray">

</hr>

### <FONT color='#0066CC'><FONT size = 4> 5. Classification Bayésienne Naïve </FONT></FONT>

<DIV align = justify>

Nous allons maintenant appliquer un classifieur Bayésien Naïf sur les axes factoriels que nous a fournis l'AFD. 
Cette approche est particulièrement efficace car nous allons combiner AFD et Bayésien naïf. 
Cette combinaison va nous permettre de réduire la dimensionnalité tout en préservant les informations discriminantes (AFD).
De plus, les axes de l'AFD sont orthogonaux, ainsi on respecte l'hypothèse d'indépendance du Bayésien naïf.
Enfin, étant donné que nous nous situons dans un problème binaire, cette combinaison est très facilement interprétable.

Maintenant nous allons utiliser la **validation croisée en 10 blocs** pour évaluer notre modèle.

```{r}

df_lda$generated <- as.factor(df_lda$generated)

# D'abord on renomme les catégories pour éviter des erreurs 
levels(df_lda$generated) <- make.names(levels(df_lda$generated))

set.seed(123)
train_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = TRUE
)

model_cv <- train(
  generated ~ ., 
  data = df_lda,
  method = "naive_bayes",
  trControl = train_control
)

print(model_cv)

```

#### <FONT color='#0066CC'><FONT size = 3> 5.1 Évaluation des performances </FONT></FONT>

Une fois que le modèle a bien été entraîné sur les 10 blocs, nous pouvons examiner les résultats moyens
en particulier l'**Accuracy** qui nous indique le pourcentage de textes correctement classifiés et 
le **Kappa** qui va prendre en compte le hasard pour rendre l'évaluation plus fiable.

```{r}

cv_accuracy <- max(model_cv$results$Accuracy)
cv_kappa <- model_cv$results$Kappa[which.max(model_cv$results$Accuracy)]

metrics <- data.frame(
  Mesure = c("Accuracy", "Kappa (CV 10-fold)"),
  Valeur = c(
    paste0(round(cv_accuracy * 100, 2), " %"),
    round(cv_kappa, 3)
  )
)

kable(metrics, caption = "Performance du Modèle Bayésien (Validation Croisée)") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center")

```

*  **Matrice de Confusion :**

```{r}

cv_predictions <- model_cv$pred

# Filtrer les prédictions finales
if (nrow(cv_predictions) > 0) {
  final_preds <- cv_predictions[cv_predictions$Resample != "Resample", ]
  
  # Vérifier qu'on a des prédictions
  if (nrow(final_preds) > 0) {
    cm_cv <- confusionMatrix(final_preds$pred, final_preds$obs)
    
    kable(cm_cv$table, caption = "Matrice de Confusion (Validation Croisée)") %>%
      kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")
  } else {
    cat("Aucune prédiction à afficher\n")
  }
} else {
  cat("Aucune prédiction disponible\n")
}

```

*  **Métriques détaillées par classe :**

```{r}

if (!is.null(cm_cv$byClass)) {
  class_metrics <- cm_cv$byClass
  detailed_metrics <- data.frame(
    Métrique = names(class_metrics),
    Valeur = round(class_metrics, 4)
  )
  
  kable(detailed_metrics, caption = "Métriques détaillées de performance") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "center")
} else {
  cat("Métriques détaillées non disponibles pour ce modèle\n")
}

```

*  **Performance par classe :**

```{r}

if (exists("cm_cv") && !is.null(cm_cv)) {
  # Table de performance par classe
  class_performance <- data.frame(
    Classe = rownames(cm_cv$table),
    Prédictions_Correctes = diag(cm_cv$table),
    Total = rowSums(cm_cv$table)
  )
  class_performance$Taux_Reussite <- round(class_performance$Prédictions_Correctes / class_performance$Total * 100, 2)
  
  kable(class_performance, caption = "Taux de réussite par classe") %>%
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position = "center")
} else {
  cat("Performance par classe non disponible\n")
}

```

*  **Visualisation des résultats :**

```{r fig.width=12, fig.height=6}

if (exists("cm_cv") && !is.null(cm_cv)) {
  # On prépare les données pour la visualisation
  cm_data <- as.data.frame(cm_cv$table)
  colnames(cm_data) <- c("Observé", "Prédit", "Fréquence")
  
  # Graphique de la matrice de confusion
  ggplot(cm_data, aes(x = Prédit, y = Observé, fill = Fréquence)) +
    geom_tile(color = "black", size = 0.5) +
    geom_text(aes(label = Fréquence), vjust = 0.5, hjust = 0.5, size = 4, fontface = "bold") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    theme_minimal() +
    theme(
      axis.title = element_text(size = 12, face = "bold"),
      plot.title = element_text(size = 13, hjust = 0.5, face = "bold"),
      axis.text = element_text(size = 11)
    ) +
    ggtitle("Matrice de Confusion - Prédictions du modèle Bayésien") +
    xlab("Classe Prédite") +
    ylab("Classe Observée")
} else {
  cat("Matrice de confusion non disponible\n")
}

```

*  **Résumé des résultats :**

```{r}

cat("Nombre total de textes traités :", nrow(df_lda), "\n")
cat("Nombre de classes (binaire) : 2 (Humain vs IA)\n")
cat("Nombre de features après sélection :", ncol(tfidf_data_reduced), "\n")
cat("Nombre d'axes discriminants :", ncol(df_lda) - 1, "\n\n")

```

*  **Performance globale :**

```{r}

cat("Accuracy globale :", round(cv_accuracy * 100, 2), "%\n")
cat("Kappa :", round(cv_kappa, 3), "\n\n")

if (exists("class_performance") && nrow(class_performance) > 0) {
  cat("--- PERFORMANCE PAR CLASSE ---\n")
  cat("Humain : ", class_performance$Taux_Reussite[1], "%\n")
  cat("IA : ", class_performance$Taux_Reussite[2], "%\n\n")
}

if (exists("cm_cv") && !is.null(cm_cv)) {
  cat("Vrais Positifs (IA correctement détectées) :", cm_cv$table[2,2], "\n")
  cat("Faux Positifs (Humain classifiés comme IA) :", cm_cv$table[1,2], "\n")
  cat("Vrais Négatifs (Humain correctement détectés) :", cm_cv$table[1,1], "\n")
  cat("Faux Négatifs (IA classifiées comme Humain) :", cm_cv$table[2,1], "\n")
}

```


Les résultats obtenus après la validation croisée sont très bons. On obtient une précision de 97.02% notre modèle réussit à classer presque parfaitement les textes. 
Le coefficient Kappa de 0.94 confirme que ce n'est pas dû à du hasard. Cela montre que la combinaison entre l'AFD qui permet de simplifier les données et le classifieur 
Bayésien fonctionnent très bien ensemble pour ce genre de projet. 

Lorsque l'on regarde la matrice de confusion on voit que le modèle reste très équilibré, 
sur un gros volume de texte, il y a 322 cas de textes humains détectés comme une IA (faux positifs) et 274 cas d'IA détectées comme humains (faux négatifs). 



</DIV>

<hr style="border: 1px solid gray">

</hr>

### <FONT color='#0066CC'><FONT size = 4> 6. Conclusion </FONT></FONT>

<DIV align = justify>

Ce projet nous a permis de développer un modèle permettant de distinguer les textes générés par une IA de ceux écrit par des humains.

Tout d'abord, nous avons effectué un prétraitement des données, en nettoyant les textes et en augmentant notre jeu de données pour atteindre un équilibre entre les classes.
Ensuite, nous avons extrait des caractéristiques à partir des textes en utilisant la vectorisation TF-IDF, ce qui nous a permis de transformer les textes en données numériques 
exploitables pour nos modèles. Nous avons ensuite appliqué l'Analyse Factorielle Discriminante (AFD) pour réduire la dimensionnalité tout en préservant les informations 
importantes, ce qui nous a permis d'identifier les axes qui séparent au mieux les textes humains des textes générés par l'IA.  
Enfin, nous avons utilisé un classifieur Bayésien Naïf sur les axes de l'AFD, avec validation croisée en 10 blocs pour évaluer la performance du modèle.

**Résultats obtenus :**

Notre modèle a atteint les performances suivantes :
- **Textes humains** : 96.8% correctement identifiés
- **Textes générés par IA** : 97.25% correctement identifiés
- Une performance équilibrée entre les deux classes, ce qui est positif

Cela signifie que notre système détecte légèrement mieux les textes IA que les textes humains, avec une petite différence.

**Conclusions finales :**

Même si les résultats sont excellents, il faut rester sceptique car les modèles de langages évoluent constamment dans le but d'imiter le style humain.
On peut aussi penser que si les résultats sont excellents ca peut être du au fait que les textes de notre jeu de données proviennent de modèles d'IA spécifiques. Peut être que dans un scénario réel avec plusieurs modèle d'IA différentes l'efficacité de notre modèle baisserait un peu. 

Cependant pour l'instant grâce à ce projet on a pu montrer que l'IA laissait des "traces" qu'il est encore possible de capturer. Cependant plus on va avancer dans le temps et plus la détection sera compliquée et restera un défi technologique dans les années à venir. 

</DIV>

<hr style="border: 1px solid gray">

</hr>
